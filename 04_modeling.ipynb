{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.python.framework import ops, tensor_shape, tensor_util\n",
    "from tensorflow.python.ops import math_ops, random_ops, array_ops\n",
    "from tensorflow.python.layers import utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "      <th>result</th>\n",
       "      <th>ERA_x</th>\n",
       "      <th>WHIP_x</th>\n",
       "      <th>h/g_x</th>\n",
       "      <th>hr/g_x</th>\n",
       "      <th>so/g_x</th>\n",
       "      <th>bb/g_x</th>\n",
       "      <th>hbp/g_x</th>\n",
       "      <th>ERA_y</th>\n",
       "      <th>WHIP_y</th>\n",
       "      <th>h/g_y</th>\n",
       "      <th>hr/g_y</th>\n",
       "      <th>so/g_y</th>\n",
       "      <th>bb/g_y</th>\n",
       "      <th>hbp/g_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.47</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>6.42</td>\n",
       "      <td>1.70</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>1.633333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.83</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>1.178571</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>6.42</td>\n",
       "      <td>1.70</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>1.633333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.35</td>\n",
       "      <td>7.214286</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>3.035714</td>\n",
       "      <td>1.464286</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>5.43</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.548387</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>1.516129</td>\n",
       "      <td>1.225806</td>\n",
       "      <td>0.096774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.71</td>\n",
       "      <td>2.01</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7.09</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.536585</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.121951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.80</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.85</td>\n",
       "      <td>1.86</td>\n",
       "      <td>5.818182</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>2.363636</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     home_score  away_score  result  ERA_x  WHIP_x     h/g_x    hr/g_x  \\\n",
       "473         5.0         4.0     1.0   5.47    1.70  2.214286  0.142857   \n",
       "474        11.0         9.0     1.0   5.83    1.58  2.571429  0.142857   \n",
       "475        16.0         3.0     1.0   4.35    1.35  7.214286  0.892857   \n",
       "476         7.0         6.0     1.0   8.71    2.01  6.200000  0.600000   \n",
       "477        10.0         3.0     1.0  10.80    1.92  2.600000  0.200000   \n",
       "\n",
       "       so/g_x    bb/g_x   hbp/g_x  ERA_y  WHIP_y     h/g_y    hr/g_y  \\\n",
       "473  1.500000  0.785714  0.357143   6.42    1.70  5.300000  0.433333   \n",
       "474  1.642857  1.178571  0.214286   6.42    1.70  5.300000  0.433333   \n",
       "475  3.035714  1.464286  0.464286   5.43    1.62  1.548387  0.290323   \n",
       "476  3.500000  2.100000  0.200000   7.09    1.77  1.536585  0.121951   \n",
       "477  0.600000  0.600000  0.000000   6.85    1.86  5.818182  0.590909   \n",
       "\n",
       "       so/g_y    bb/g_y   hbp/g_y  \n",
       "473  2.833333  1.633333  0.166667  \n",
       "474  2.833333  1.633333  0.166667  \n",
       "475  1.516129  1.225806  0.096774  \n",
       "476  0.780488  0.487805  0.121951  \n",
       "477  2.363636  2.181818  0.227273  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"./data/model_test.csv\", encoding='utf-8')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.result != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = df.ix[:, 3:]\n",
    "y = pd.get_dummies(df.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages\n",
      "ERA_x      9.489086e-18\n",
      "WHIP_x     3.083953e-18\n",
      "h/g_x     -1.233581e-17\n",
      "hr/g_x     4.175198e-17\n",
      "so/g_x     4.222643e-17\n",
      "bb/g_x     4.791988e-17\n",
      "hbp/g_x    2.324826e-17\n",
      "ERA_y     -1.945263e-17\n",
      "WHIP_y    -3.131398e-17\n",
      "h/g_y      1.335589e-16\n",
      "hr/g_y     7.472655e-17\n",
      "so/g_y    -7.792912e-17\n",
      "bb/g_y     1.511137e-16\n",
      "hbp/g_y   -1.135132e-16\n",
      "dtype: float64\n",
      "\n",
      " Deviations\n",
      "ERA_x      1.0\n",
      "WHIP_x     1.0\n",
      "h/g_x      1.0\n",
      "hr/g_x     1.0\n",
      "so/g_x     1.0\n",
      "bb/g_x     1.0\n",
      "hbp/g_x    1.0\n",
      "ERA_y      1.0\n",
      "WHIP_y     1.0\n",
      "h/g_y      1.0\n",
      "hr/g_y     1.0\n",
      "so/g_y     1.0\n",
      "bb/g_y     1.0\n",
      "hbp/g_y    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for feature in X.columns:\n",
    "    X[feature] = (X[feature] - X[feature].mean())/X[feature].std()\n",
    "    \n",
    "print(\"Averages\")\n",
    "print(X.mean())\n",
    "\n",
    "print(\"\\n Deviations\")\n",
    "\n",
    "print(pow(X.std(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327, 14) (327, 2)\n",
      "(141, 14) (141, 2)\n"
     ]
    }
   ],
   "source": [
    "# Generate Traning and Validation Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Convert to np arrays so that we can use with Tensorflow\n",
    "X_train = np.array(X_train).astype(np.float32)\n",
    "X_test  = np.array(X_test).astype(np.float32)\n",
    "y_train = np.array(y_train).astype(np.float32)\n",
    "y_test  = np.array(y_test).astype(np.float32)\n",
    "\n",
    "print(np.shape(X_train), np.shape(y_train))\n",
    "print(np.shape(X_test), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 33\n",
    "n_epochs = 10000\n",
    "\n",
    "# Create placeholders for features and labels\n",
    "X = tf.placeholder(tf.float32, [None, 14], name='X_placeholder')\n",
    "Y = tf.placeholder(tf.float32, [None, 2], name='Y_placeholder')\n",
    "\n",
    "# Create weights and bias\n",
    "w = tf.Variable(tf.random_normal(shape=[14, 2], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 2], name='bias'))\n",
    "\n",
    "# Build model that returns the logits\n",
    "logits = tf.matmul(X, w) + b\n",
    "\n",
    "# Define log loss function ==> cross entropy of softmax of logits\n",
    "y = tf.nn.softmax(logits)\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.7092207446694374\n",
      "Average loss epoch 1000: 0.6675850674510002\n",
      "Average loss epoch 2000: 0.6693468168377876\n",
      "Average loss epoch 3000: 0.6708036884665489\n",
      "Average loss epoch 4000: 0.6705411821603775\n",
      "Average loss epoch 5000: 0.6707606837153435\n",
      "Average loss epoch 6000: 0.6720704808831215\n",
      "Average loss epoch 7000: 0.6639352217316628\n",
      "Average loss epoch 8000: 0.6681230887770653\n",
      "Average loss epoch 9000: 0.6676737368106842\n",
      "Optimization Finished\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_batches = int(len(X_train)/batch_size)\n",
    "    \n",
    "    for i in range(n_epochs): # train the model\n",
    "        \n",
    "        # shuffle X, y\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = X_train[j*batch_size:(j+1)*batch_size], y_train[j*batch_size:(j+1)*batch_size]\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "            total_loss += loss_batch\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "    print('Optimization Finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(327, 14), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(327, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "training_size = X_train.shape[1]\n",
    "test_size = X_test.shape[1]\n",
    "num_features = 14\n",
    "num_labels = 2\n",
    "\n",
    "\n",
    "num_hidden = 7\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_set    = tf.constant(X_train)\n",
    "    tf_train_labels = tf.constant(y_train)\n",
    "    tf_valid_set    = tf.constant(X_test)\n",
    " \n",
    "    \n",
    "    print(tf_train_set)\n",
    "    print(tf_train_labels)\n",
    "    \n",
    "    ## Note, since there is only 1 layer there are actually no hidden layers... but if there were\n",
    "    ## there would be num_hidden\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([num_features, num_hidden]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "    ## tf.zeros Automaticaly adjusts rows to input data batch size\n",
    "    bias_1 = tf.Variable(tf.zeros([num_hidden]))\n",
    "    bias_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_set , weights_1 ) + bias_1\n",
    "    rel_1 = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(rel_1, weights_2) + bias_2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_2, labels=tf_train_labels))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(.005).minimize(loss)\n",
    "    \n",
    "    \n",
    "    ## Training prediction\n",
    "    predict_train = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Validation prediction\n",
    "    logits_1_val = tf.matmul(tf_valid_set, weights_1) + bias_1\n",
    "    rel_1_val    = tf.nn.softmax(logits_1_val)\n",
    "    logits_2_val = tf.matmul(rel_1_val, weights_2) + bias_2\n",
    "    predict_valid = tf.nn.softmax(logits_2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14003\n",
      "Loss at step 0: 3.140029\n",
      "Training accuracy: 52.0%\n",
      "Testing accuracy: 57.4%\n",
      "Loss at step 10000: 0.663013\n",
      "Training accuracy: 60.6%\n",
      "Testing accuracy: 56.7%\n",
      "Loss at step 20000: 0.647391\n",
      "Training accuracy: 64.5%\n",
      "Testing accuracy: 56.7%\n",
      "Loss at step 30000: 0.628001\n",
      "Training accuracy: 66.1%\n",
      "Testing accuracy: 57.4%\n",
      "Loss at step 40000: 0.609186\n",
      "Training accuracy: 68.8%\n",
      "Testing accuracy: 58.2%\n",
      "Loss at step 50000: 0.590150\n",
      "Training accuracy: 67.9%\n",
      "Testing accuracy: 59.6%\n",
      "Loss at step 60000: 0.573551\n",
      "Training accuracy: 68.5%\n",
      "Testing accuracy: 59.6%\n",
      "Loss at step 70000: 0.555376\n",
      "Training accuracy: 71.3%\n",
      "Testing accuracy: 58.9%\n",
      "Loss at step 80000: 0.541738\n",
      "Training accuracy: 72.5%\n",
      "Testing accuracy: 55.3%\n",
      "Loss at step 90000: 0.533926\n",
      "Training accuracy: 72.5%\n",
      "Testing accuracy: 56.0%\n",
      "Loss at step 99999: 0.526665\n",
      "Training accuracy: 72.5%\n",
      "Testing accuracy: 61.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100000\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(loss.eval())\n",
    "    test_accuracy = []\n",
    "    for step in range(num_steps):\n",
    "        _,l, predictions = session.run([optimizer, loss, predict_train])\n",
    "        test_accuracy.append(accuracy(predict_valid.eval(), y_test))\n",
    "        \n",
    "        if (step % 10000 == 0 or step == num_steps-1):\n",
    "#               print(predictions[3:6])\n",
    "              print('Loss at step %d: %f' % (step, l))\n",
    "              print('Training accuracy: %.1f%%' % accuracy(predictions, y_train[:, :]))\n",
    "              print('Testing accuracy: %.1f%%' % accuracy(predict_valid.eval(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.248226950354606"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
